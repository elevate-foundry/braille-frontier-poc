"""
Download a free text corpus for training.

Uses HuggingFace datasets library to get TinyStories - high quality 
short stories generated by GPT-4 for training small language models.

This is effectively pre-distilled data from a large model.
"""

import json
from pathlib import Path
import argparse


def download_tinystories(output_path: Path, max_samples: int = 50000):
    """Download TinyStories dataset using HuggingFace datasets."""
    try:
        from datasets import load_dataset
    except ImportError:
        print("Installing datasets library...")
        import subprocess
        subprocess.check_call(["pip", "install", "datasets", "-q"])
        from datasets import load_dataset
    
    print("Downloading TinyStories dataset...")
    print("(This is GPT-4 generated data - effectively distilled knowledge)")
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    dataset = load_dataset("roneneldan/TinyStories", split="train", streaming=True)
    
    samples = []
    for i, item in enumerate(dataset):
        text = item.get("text", "")
        if len(text) > 50:
            samples.append(text)
        
        if len(samples) >= max_samples:
            break
        
        if len(samples) % 1000 == 0 and len(samples) > 0:
            print(f"  Downloaded {len(samples):,} stories...")
    
    print(f"Downloaded {len(samples):,} stories")
    
    with open(output_path, "w") as f:
        for story in samples:
            record = {"text": story, "model": "gpt-4-tinystories"}
            f.write(json.dumps(record) + "\n")
    
    print(f"Saved to {output_path}")
    return len(samples)


def main():
    parser = argparse.ArgumentParser(description="Download TinyStories corpus")
    parser.add_argument("--samples", type=int, default=50000, help="Max samples")
    parser.add_argument("--output", type=str, default="distill/data/generated.jsonl")
    args = parser.parse_args()
    
    download_tinystories(Path(args.output), args.samples)


if __name__ == "__main__":
    main()
